{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from graph_construction import construct_graph\n",
    "from tcn_autoencoder import train_and_extract_features\n",
    "from data_splitting import create_train_val_tf, generate_train_val_kfold\n",
    "from transformer_model import train_model, test_model\n",
    "from test_split import split_edges, split_tfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"mESC\"\n",
    "adata = sc.read(\"./\" +file_name + \".h5ad\")\n",
    "\n",
    "filtered_refnet = adata.uns[\"grn\"]\n",
    "fea_df = pd.DataFrame(adata.uns[\"gpt_emb\"])\n",
    "fea_df[\"Gene\"] = adata.var_names\n",
    "X_norm = adata.X.T\n",
    "tfs = np.unique(filtered_refnet[\"Gene1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[500, 1536], edge_index=[2, 2347])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orig, tfs_index, le = construct_graph(filtered_refnet, adata.var_names,fea_df, tfs)\n",
    "data_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zfd297/miniconda3/envs/infer_cau/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.0245\n",
      "Epoch [2/200], Train Loss: 0.0242\n",
      "Epoch [3/200], Train Loss: 0.0241\n",
      "Epoch [4/200], Train Loss: 0.0240\n",
      "Epoch [5/200], Train Loss: 0.0239\n",
      "Epoch [6/200], Train Loss: 0.0239\n",
      "Epoch [7/200], Train Loss: 0.0239\n",
      "Epoch [8/200], Train Loss: 0.0238\n",
      "Epoch [9/200], Train Loss: 0.0237\n",
      "Epoch [10/200], Train Loss: 0.0235\n",
      "Epoch [11/200], Train Loss: 0.0233\n",
      "Epoch [12/200], Train Loss: 0.0228\n",
      "Epoch [13/200], Train Loss: 0.0219\n",
      "Epoch [14/200], Train Loss: 0.0192\n",
      "Epoch [15/200], Train Loss: 0.0120\n",
      "Epoch [16/200], Train Loss: 0.0068\n",
      "Epoch [17/200], Train Loss: 0.0056\n",
      "Epoch [18/200], Train Loss: 0.0051\n",
      "Epoch [19/200], Train Loss: 0.0046\n",
      "Epoch [20/200], Train Loss: 0.0044\n",
      "Epoch [21/200], Train Loss: 0.0043\n",
      "Epoch [22/200], Train Loss: 0.0041\n",
      "Epoch [23/200], Train Loss: 0.0040\n",
      "Epoch [24/200], Train Loss: 0.0038\n",
      "Epoch [25/200], Train Loss: 0.0036\n",
      "Epoch [26/200], Train Loss: 0.0035\n",
      "Epoch [27/200], Train Loss: 0.0034\n",
      "Epoch [28/200], Train Loss: 0.0033\n",
      "Epoch [29/200], Train Loss: 0.0031\n",
      "Epoch [30/200], Train Loss: 0.0030\n",
      "Epoch [31/200], Train Loss: 0.0029\n",
      "Epoch [32/200], Train Loss: 0.0027\n",
      "Epoch [33/200], Train Loss: 0.0026\n",
      "Epoch [34/200], Train Loss: 0.0024\n",
      "Epoch [35/200], Train Loss: 0.0022\n",
      "Epoch [36/200], Train Loss: 0.0020\n",
      "Epoch [37/200], Train Loss: 0.0018\n",
      "Epoch [38/200], Train Loss: 0.0016\n",
      "Epoch [39/200], Train Loss: 0.0014\n",
      "Epoch [40/200], Train Loss: 0.0012\n",
      "Epoch [41/200], Train Loss: 0.0011\n",
      "Epoch [42/200], Train Loss: 0.0010\n",
      "Epoch [43/200], Train Loss: 0.0009\n",
      "Epoch [44/200], Train Loss: 0.0009\n",
      "Epoch [45/200], Train Loss: 0.0008\n",
      "Epoch [46/200], Train Loss: 0.0008\n",
      "Epoch [47/200], Train Loss: 0.0008\n",
      "Epoch [48/200], Train Loss: 0.0007\n",
      "Epoch [49/200], Train Loss: 0.0007\n",
      "Epoch [50/200], Train Loss: 0.0007\n",
      "Epoch [51/200], Train Loss: 0.0006\n",
      "Epoch [52/200], Train Loss: 0.0006\n",
      "Epoch [53/200], Train Loss: 0.0006\n",
      "Epoch [54/200], Train Loss: 0.0006\n",
      "Epoch [55/200], Train Loss: 0.0006\n",
      "Epoch [56/200], Train Loss: 0.0006\n",
      "Epoch [57/200], Train Loss: 0.0005\n",
      "Epoch [58/200], Train Loss: 0.0005\n",
      "Epoch [59/200], Train Loss: 0.0005\n",
      "Epoch [60/200], Train Loss: 0.0005\n",
      "Epoch [61/200], Train Loss: 0.0005\n",
      "Epoch [62/200], Train Loss: 0.0005\n",
      "Epoch [63/200], Train Loss: 0.0004\n",
      "Epoch [64/200], Train Loss: 0.0004\n",
      "Epoch [65/200], Train Loss: 0.0004\n",
      "Epoch [66/200], Train Loss: 0.0004\n",
      "Epoch [67/200], Train Loss: 0.0004\n",
      "Epoch [68/200], Train Loss: 0.0004\n",
      "Epoch [69/200], Train Loss: 0.0004\n",
      "Epoch [70/200], Train Loss: 0.0004\n",
      "Epoch [71/200], Train Loss: 0.0003\n",
      "Epoch [72/200], Train Loss: 0.0003\n",
      "Epoch [73/200], Train Loss: 0.0003\n",
      "Epoch [74/200], Train Loss: 0.0003\n",
      "Epoch [75/200], Train Loss: 0.0003\n",
      "Epoch [76/200], Train Loss: 0.0003\n",
      "Epoch [77/200], Train Loss: 0.0003\n",
      "Epoch [78/200], Train Loss: 0.0003\n",
      "Epoch [79/200], Train Loss: 0.0003\n",
      "Epoch [80/200], Train Loss: 0.0003\n",
      "Epoch [81/200], Train Loss: 0.0003\n",
      "Epoch [82/200], Train Loss: 0.0003\n",
      "Epoch [83/200], Train Loss: 0.0003\n",
      "Epoch [84/200], Train Loss: 0.0003\n",
      "Epoch [85/200], Train Loss: 0.0003\n",
      "Epoch [86/200], Train Loss: 0.0002\n",
      "Epoch [87/200], Train Loss: 0.0002\n",
      "Epoch [88/200], Train Loss: 0.0002\n",
      "Epoch [89/200], Train Loss: 0.0002\n",
      "Epoch [90/200], Train Loss: 0.0002\n",
      "Epoch [91/200], Train Loss: 0.0002\n",
      "Epoch [92/200], Train Loss: 0.0002\n",
      "Epoch [93/200], Train Loss: 0.0002\n",
      "Epoch [94/200], Train Loss: 0.0002\n",
      "Epoch [95/200], Train Loss: 0.0002\n",
      "Epoch [96/200], Train Loss: 0.0002\n",
      "Epoch [97/200], Train Loss: 0.0002\n",
      "Epoch [98/200], Train Loss: 0.0002\n",
      "Epoch [99/200], Train Loss: 0.0002\n",
      "Epoch [100/200], Train Loss: 0.0002\n",
      "Epoch [101/200], Train Loss: 0.0002\n",
      "Epoch [102/200], Train Loss: 0.0002\n",
      "Epoch [103/200], Train Loss: 0.0002\n",
      "Epoch [104/200], Train Loss: 0.0002\n",
      "Epoch [105/200], Train Loss: 0.0002\n",
      "Epoch [106/200], Train Loss: 0.0002\n",
      "Epoch [107/200], Train Loss: 0.0002\n",
      "Epoch [108/200], Train Loss: 0.0002\n",
      "Epoch [109/200], Train Loss: 0.0002\n",
      "Epoch [110/200], Train Loss: 0.0002\n",
      "Epoch [111/200], Train Loss: 0.0002\n",
      "Epoch [112/200], Train Loss: 0.0002\n",
      "Epoch [113/200], Train Loss: 0.0002\n",
      "Epoch [114/200], Train Loss: 0.0002\n",
      "Epoch [115/200], Train Loss: 0.0002\n",
      "Epoch [116/200], Train Loss: 0.0002\n",
      "Epoch [117/200], Train Loss: 0.0001\n",
      "Epoch [118/200], Train Loss: 0.0001\n",
      "Epoch [119/200], Train Loss: 0.0001\n",
      "Epoch [120/200], Train Loss: 0.0001\n",
      "Epoch [121/200], Train Loss: 0.0001\n",
      "Epoch [122/200], Train Loss: 0.0001\n",
      "Epoch [123/200], Train Loss: 0.0001\n",
      "Epoch [124/200], Train Loss: 0.0001\n",
      "Epoch [125/200], Train Loss: 0.0001\n",
      "Epoch [126/200], Train Loss: 0.0001\n",
      "Epoch [127/200], Train Loss: 0.0001\n",
      "Epoch [128/200], Train Loss: 0.0001\n",
      "Epoch [129/200], Train Loss: 0.0001\n",
      "Epoch [130/200], Train Loss: 0.0001\n",
      "Epoch [131/200], Train Loss: 0.0001\n",
      "Epoch [132/200], Train Loss: 0.0001\n",
      "Epoch [133/200], Train Loss: 0.0001\n",
      "Epoch [134/200], Train Loss: 0.0001\n",
      "Epoch [135/200], Train Loss: 0.0001\n",
      "Epoch [136/200], Train Loss: 0.0001\n",
      "Epoch [137/200], Train Loss: 0.0001\n",
      "Epoch [138/200], Train Loss: 0.0001\n",
      "Epoch [139/200], Train Loss: 0.0001\n",
      "Epoch [140/200], Train Loss: 0.0001\n",
      "Epoch [141/200], Train Loss: 0.0001\n",
      "Epoch [142/200], Train Loss: 0.0001\n",
      "Epoch [143/200], Train Loss: 0.0001\n",
      "Epoch [144/200], Train Loss: 0.0001\n",
      "Epoch [145/200], Train Loss: 0.0001\n",
      "Epoch [146/200], Train Loss: 0.0001\n",
      "Epoch [147/200], Train Loss: 0.0001\n",
      "Epoch [148/200], Train Loss: 0.0001\n",
      "Epoch [149/200], Train Loss: 0.0001\n",
      "Epoch [150/200], Train Loss: 0.0001\n",
      "Epoch [151/200], Train Loss: 0.0001\n",
      "Epoch [152/200], Train Loss: 0.0001\n",
      "Epoch [153/200], Train Loss: 0.0001\n",
      "Epoch [154/200], Train Loss: 0.0001\n",
      "Epoch [155/200], Train Loss: 0.0001\n",
      "Epoch [156/200], Train Loss: 0.0001\n",
      "Epoch [157/200], Train Loss: 0.0001\n",
      "Epoch [158/200], Train Loss: 0.0001\n",
      "Epoch [159/200], Train Loss: 0.0001\n",
      "Epoch [160/200], Train Loss: 0.0001\n",
      "Epoch [161/200], Train Loss: 0.0001\n",
      "Epoch [162/200], Train Loss: 0.0001\n",
      "Epoch [163/200], Train Loss: 0.0001\n",
      "Epoch [164/200], Train Loss: 0.0001\n",
      "Epoch [165/200], Train Loss: 0.0001\n",
      "Epoch [166/200], Train Loss: 0.0001\n",
      "Epoch [167/200], Train Loss: 0.0001\n",
      "Epoch [168/200], Train Loss: 0.0001\n",
      "Epoch [169/200], Train Loss: 0.0001\n",
      "Epoch [170/200], Train Loss: 0.0001\n",
      "Epoch [171/200], Train Loss: 0.0001\n",
      "Epoch [172/200], Train Loss: 0.0001\n",
      "Epoch [173/200], Train Loss: 0.0001\n",
      "Epoch [174/200], Train Loss: 0.0001\n",
      "Epoch [175/200], Train Loss: 0.0001\n",
      "Epoch [176/200], Train Loss: 0.0001\n",
      "Epoch [177/200], Train Loss: 0.0001\n",
      "Epoch [178/200], Train Loss: 0.0001\n",
      "Epoch [179/200], Train Loss: 0.0001\n",
      "Epoch [180/200], Train Loss: 0.0001\n",
      "Epoch [181/200], Train Loss: 0.0001\n",
      "Epoch [182/200], Train Loss: 0.0001\n",
      "Epoch [183/200], Train Loss: 0.0001\n",
      "Epoch [184/200], Train Loss: 0.0001\n",
      "Epoch [185/200], Train Loss: 0.0001\n",
      "Epoch [186/200], Train Loss: 0.0001\n",
      "Epoch [187/200], Train Loss: 0.0001\n",
      "Epoch [188/200], Train Loss: 0.0001\n",
      "Epoch [189/200], Train Loss: 0.0001\n",
      "Epoch [190/200], Train Loss: 0.0001\n",
      "Epoch [191/200], Train Loss: 0.0001\n",
      "Epoch [192/200], Train Loss: 0.0001\n",
      "Epoch [193/200], Train Loss: 0.0001\n",
      "Epoch [194/200], Train Loss: 0.0001\n",
      "Epoch [195/200], Train Loss: 0.0001\n",
      "Epoch [196/200], Train Loss: 0.0001\n",
      "Epoch [197/200], Train Loss: 0.0001\n",
      "Epoch [198/200], Train Loss: 0.0001\n",
      "Epoch [199/200], Train Loss: 0.0001\n",
      "Epoch [200/200], Train Loss: 0.0001\n",
      "Training complete.\n",
      "Features extracted. Shape: torch.Size([500, 36, 421])\n",
      "Reduced features shape: torch.Size([500, 36])\n",
      "torch.Size([500, 36])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tcauto_model, features = train_and_extract_features(X_norm,learning_rate=0.001, weight_decay=1e-4)\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[500, 1536], edge_index=[2, 2347], x_additional=[500, 36])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_features_df = pd.DataFrame(features.cpu().numpy())\n",
    "reduced_features_df[\"Gene\"] = adata.var_names\n",
    "reduced_features_df\n",
    "\n",
    "# create features of nodes\n",
    "node_features_1 = fea_df.set_index('Gene').reindex(le.classes_).fillna(0).values\n",
    "node_features_2 = reduced_features_df.set_index('Gene').reindex(le.classes_).fillna(0).values\n",
    "\n",
    "\n",
    "x = torch.tensor(node_features_1, dtype=torch.float)\n",
    "x_additional = torch.tensor(node_features_2, dtype=torch.float)\n",
    "# build graph object\n",
    "data = Data(x=x, x_additional=x_additional, edge_index=data_orig.edge_index)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_sets_kfold = generate_train_val_kfold(data,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286])),\n",
       " (Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 704], edge_label=[704]),\n",
       "  Data(x=[500, 1536], x_additional=[500, 36], edge_label_index=[2, 3286], edge_label=[3286]))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_sets_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "--------------\n",
      "Fold 2:\n",
      "--------------\n",
      "Fold 3:\n",
      "--------------\n",
      "Fold 4:\n",
      "--------------\n",
      "Fold 5:\n",
      "--------------\n",
      "Fold 6:\n",
      "--------------\n",
      "Fold 7:\n",
      "--------------\n",
      "Fold 8:\n",
      "--------------\n",
      "Fold 9:\n",
      "--------------\n",
      "Fold 10:\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9666951940880183, 0.9637874247670541),\n",
       " (0.9605497388406883, 0.9691244398393895),\n",
       " (0.9555319653212806, 0.9596006444278072),\n",
       " (0.9534135787739697, 0.9591827813750138),\n",
       " (0.9703399510623356, 0.974674461343778),\n",
       " (0.9776808524388558, 0.9805952992629237),\n",
       " (0.9436331563778506, 0.9492533476276084),\n",
       " (0.9554744009407463, 0.9545422047240011),\n",
       " (0.9701376971790318, 0.9723343541589413),\n",
       " (0.9783275368297054, 0.98069124476884)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_val_sets = train_val_sets_kfold\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold, (train_data, val_data) in enumerate(train_val_sets):\n",
    "    print(f'Fold {fold + 1}:')\n",
    "    \n",
    "    # training\n",
    "    model = train_model(\n",
    "        train_data, \n",
    "        \n",
    "        hidden_channels=64, \n",
    "        num_heads=16, \n",
    "        dropout=0.5, \n",
    "        lr=0.000005, \n",
    "        weight_decay=1e-3, \n",
    "        num_epochs=200, \n",
    "        print_interval=10\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'fold': fold + 1,\n",
    "       \n",
    "        'model': model\n",
    "    })\n",
    "    \n",
    "    print('--------------')\n",
    "\n",
    "\n",
    "\n",
    "model_list = [r['model'] for r in results]\n",
    "test_auc = [test_model(i, val_data) for i in model_list]\n",
    "test_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer_cau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
